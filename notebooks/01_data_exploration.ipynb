{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Article Frame Analysis - Data Exploration\n",
    "\n",
    "This notebook explores the news articles dataset and demonstrates the frame detection system.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yourusername/spam_news/blob/main/notebooks/01_data_exploration.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect if we're running in Google Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Google Colab\")\n",
    "    # Clone the repository\n",
    "    !git clone https://github.com/yourusername/spam_news.git\n",
    "    %cd spam_news\n",
    "    \n",
    "    # Install requirements\n",
    "    !pip install -q -r requirements.txt\n",
    "    \n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "else:\n",
    "    print(\"Running locally\")\n",
    "    # Ensure we're in the project root\n",
    "    import os\n",
    "    if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "        os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Detection and Setup\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"No GPU available, using CPU\")\n",
    "\n",
    "# Set device for transformers\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0' if torch.cuda.is_available() else ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure data paths\n",
    "if IN_COLAB:\n",
    "    # Update this path to your Google Drive folder containing the full dataset\n",
    "    FULL_DATA_PATH = Path('/content/drive/MyDrive/spam_news_data/articles.json')\n",
    "    SAMPLE_DATA_PATH = Path('data/sample_articles.json')\n",
    "else:\n",
    "    FULL_DATA_PATH = Path('data/articles.json')\n",
    "    SAMPLE_DATA_PATH = Path('data/sample_articles.json')\n",
    "\n",
    "# Load data\n",
    "if FULL_DATA_PATH.exists():\n",
    "    print(f\"Loading full dataset from {FULL_DATA_PATH}\")\n",
    "    with open(FULL_DATA_PATH, 'r') as f:\n",
    "        articles = json.load(f)\n",
    "else:\n",
    "    print(f\"Full dataset not found. Loading sample data from {SAMPLE_DATA_PATH}\")\n",
    "    with open(SAMPLE_DATA_PATH, 'r') as f:\n",
    "        articles = json.load(f)\n",
    "\n",
    "print(f\"\\nLoaded {len(articles)} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for easier analysis\n",
    "df = pd.DataFrame(articles)\n",
    "\n",
    "# Display basic info\n",
    "print(\"Dataset Overview:\")\n",
    "print(f\"Total articles: {len(df)}\")\n",
    "print(f\"\\nSources: {df['source'].value_counts().to_dict()}\")\n",
    "print(f\"\\nDate range: {df['date'].min()} to {df['date'].max()}\")\n",
    "\n",
    "# Show first few articles\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore Human Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract human coding data\n",
    "def extract_frame_counts(df):\n",
    "    frame_types = ['underrepresentation', 'overrepresentation', 'obstacles', 'successes']\n",
    "    demographic_groups = ['women', 'men', 'white_women', 'white_men', 'women_of_color', 'men_of_color']\n",
    "    \n",
    "    results = []\n",
    "    for idx, row in df.iterrows():\n",
    "        coding = row['human_coding']\n",
    "        for frame in frame_types:\n",
    "            if frame in coding:\n",
    "                for group, count in coding[frame].items():\n",
    "                    results.append({\n",
    "                        'article_id': row['article_id'],\n",
    "                        'frame': frame,\n",
    "                        'demographic': group,\n",
    "                        'count': count\n",
    "                    })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "coding_df = extract_frame_counts(df)\n",
    "print(\"Human Coding Summary:\")\n",
    "print(coding_df.groupby(['frame', 'demographic'])['count'].sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze article lengths\n",
    "df['content_length'] = df['content'].str.len()\n",
    "df['word_count'] = df['content'].str.split().str.len()\n",
    "\n",
    "print(\"Article Length Statistics:\")\n",
    "print(f\"Average character count: {df['content_length'].mean():.0f}\")\n",
    "print(f\"Average word count: {df['word_count'].mean():.0f}\")\n",
    "print(f\"Min/Max words: {df['word_count'].min()} / {df['word_count'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for frame-related keywords\n",
    "frame_keywords = {\n",
    "    'underrepresentation': ['underrepresented', 'lower rates', 'less than', 'only', 'just'],\n",
    "    'overrepresentation': ['overrepresented', 'dominate', 'majority', 'most'],\n",
    "    'obstacles': ['barrier', 'ceiling', 'discrimination', 'harder', 'challenges'],\n",
    "    'successes': ['first', 'breakthrough', 'achievement', 'milestone', 'appointed']\n",
    "}\n",
    "\n",
    "demographic_keywords = {\n",
    "    'women': ['women', 'woman', 'female'],\n",
    "    'men': ['men', 'man', 'male'],\n",
    "    'white': ['white', 'caucasian'],\n",
    "    'poc': ['black', 'african american', 'hispanic', 'latino', 'asian', 'people of color']\n",
    "}\n",
    "\n",
    "# Count keyword occurrences\n",
    "for frame, keywords in frame_keywords.items():\n",
    "    pattern = '|'.join(keywords)\n",
    "    df[f'{frame}_keywords'] = df['content'].str.lower().str.count(pattern)\n",
    "\n",
    "print(\"Frame Keyword Frequencies:\")\n",
    "for frame in frame_keywords:\n",
    "    print(f\"{frame}: {df[f'{frame}_keywords'].sum()} occurrences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Frame distribution\n",
    "if len(coding_df) > 0:\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Frame counts\n",
    "    frame_counts = coding_df.groupby('frame')['count'].sum()\n",
    "    frame_counts.plot(kind='bar', ax=ax[0])\n",
    "    ax[0].set_title('Total Frame Counts in Dataset')\n",
    "    ax[0].set_xlabel('Frame Type')\n",
    "    ax[0].set_ylabel('Total Count')\n",
    "    \n",
    "    # Demographic distribution\n",
    "    demo_counts = coding_df.groupby('demographic')['count'].sum()\n",
    "    demo_counts.plot(kind='bar', ax=ax[1])\n",
    "    ax[1].set_title('Frame Counts by Demographic Group')\n",
    "    ax[1].set_xlabel('Demographic Group')\n",
    "    ax[1].set_ylabel('Total Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save progress to prevent data loss\n",
    "if IN_COLAB:\n",
    "    # Save to Google Drive\n",
    "    save_path = Path('/content/drive/MyDrive/spam_news_data/preprocessed_data.pkl')\n",
    "    save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "else:\n",
    "    save_path = Path('data/preprocessed_data.pkl')\n",
    "\n",
    "# Save DataFrame with additional features\n",
    "df.to_pickle(save_path)\n",
    "print(f\"Preprocessed data saved to {save_path}\")\n",
    "\n",
    "# Also save coding summary\n",
    "coding_df.to_csv(save_path.with_suffix('.csv'), index=False)\n",
    "print(f\"Coding summary saved to {save_path.with_suffix('.csv')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Continue to `02_train_model.ipynb` to train the frame detection model\n",
    "2. Use `03_evaluate_model.ipynb` to evaluate model performance\n",
    "3. Try `demo_analysis.ipynb` for a quick demo of the complete pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}